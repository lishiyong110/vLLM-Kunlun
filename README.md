![vLLM Kunlun Logo](vllm_kunlun/patches/vLLM_Kunlun.jpg)

<p align="center">
  <a href="https://vllm-kunlun.readthedocs.io/en/latest/"><b>ğŸ“– Documentation</b></a> |
  <a href="https://vllm-kunlun.readthedocs.io/en/latest/quick_start.html"><b>ğŸš€ Quick Start</b></a> |
  <a href="https://vllm-kunlun.readthedocs.io/en/latest/installation.html"><b>ğŸ“¦ Installation</b></a> |
  <a href="https://join.slack.com/t/vllm-kunlun/shared_invite/zt-3iinb8u5z-FcqZKbNNdMJ_32fHmipzvw"><b>ğŸ’¬ Slack</b></a>
</p>

<p align="center">
  <img alt="GitHub License" src="https://img.shields.io/github/license/baidu/vLLM-Kunlun">
  <img alt="GitHub Stars" src="https://img.shields.io/github/stars/baidu/vLLM-Kunlun">
  <img alt="GitHub Forks" src="https://img.shields.io/github/forks/baidu/vLLM-Kunlun">
  <img alt="GitHub Issues" src="https://img.shields.io/github/issues/baidu/vLLM-Kunlun">
  <img alt="Python Version" src="https://img.shields.io/badge/python-%3E%3D3.10-blue">
</p>

---

## Latest News ğŸ”¥

- [2026/02] ğŸ§  **GLM model family support** â€” Added GLM5, GLM-4.7 MTP (Multi-Token Prediction), and GLM-47 tool parser with thinking/non-thinking mode toggle
- [2026/02] âš¡ **Performance optimizations** â€” Fused MoE with small batches, optimized attention metadata building, Multi-LoRA inference achieves 80%+ of non-LoRA performance
- [2026/02] ğŸ”§ **DeepSeek-V3.2 MTP support** â€” Added MTP (Multi-Token Prediction) for DeepSeek-V3.2, with RoPE and decoding stage kernel optimizations
- [2026/01] ğŸ”¢ **New quantization methods** â€” Support for compressed-tensors W4A16, AWQ MoE W4A16, and DeepSeek-V3.2 W8A8 quantization
- [2026/01] ğŸ› ï¸ **CI/CD overhaul** â€” Added E2E tests, unit test CI, ruff format checks, and modular CI workflow refactoring
- [2025/12] ğŸ‰ **v0.11.0rc1 released** â€” Added Qwen3-Omni, Qwen3-Next, Seed-OSS support ([Release Notes](https://github.com/baidu/vLLM-Kunlun/releases/tag/v0.11.0rc1))
- [2025/12] ğŸ“¦ **v0.10.1.1 released** â€” 5+ multimodal models, AWQ/GPTQ quantization for dense models, Piecewise CUDA Graph, vLLM V1 engine, Flash-Infer Top-K/Top-P sampling with 10-100Ã— speedup ([Release Notes](https://github.com/baidu/vLLM-Kunlun/releases/tag/v0.10.1.1))
- [2025/12] ğŸŒŸ Initial release of vLLM Kunlun â€” Open sourced on Dec 8, 2025

---

## Overview

**vLLM Kunlun** (`vllm-kunlun`) is a community-maintained hardware plugin designed to seamlessly run [vLLM](https://github.com/vllm-project/vllm) on the **Kunlun XPU**. It is the recommended approach for integrating the Kunlun backend within the vLLM community, adhering to the principles outlined in the [RFC Hardware Pluggable](https://github.com/vllm-project/vllm/issues/11162).

This plugin provides a hardware-pluggable interface that decouples the integration of the Kunlun XPU with vLLM. By utilizing vLLM Kunlun, popular open-source models â€” including Transformer-like, Mixture-of-Expert (MoE), Embedding, and Multi-modal LLMs â€” can run effortlessly on the Kunlun XPU.

### âœ¨ Key Features

- **Seamless Plugin Integration** â€” Works as a standard vLLM platform plugin via Python entry points, no need to modify vLLM source code
- **Broad Model Support** â€” Supports 15+ mainstream LLMs including Qwen, Llama, DeepSeek, Kimi-K2, and multimodal models
- **Quantization Support** â€” INT8 and other quantization methods for MoE and dense models
- **LoRA Fine-Tuning** â€” LoRA adapter support for Qwen series models
- **Piecewise Kunlun Graph** â€” Hardware-accelerated graph optimization for high-performance inference
- **FlashMLA Attention** â€” Optimized multi-head latent attention for DeepSeek MLA architectures
- **Tensor Parallelism** â€” Multi-device parallel inference with distributed execution support
- **OpenAI-Compatible API** â€” Serve models with the standard OpenAI API interface

---

## Prerequisites

- **Hardware**: Kunlun3 P800
- **OS**: Ubuntu 22.04
- **Software**:
  - Python >= 3.10
  - PyTorch >= 2.5.1
  - vLLM (same version as vllm-kunlun)
  - transformers >= 4.57.0

---

## Supported Models

### Generative Models

| Model | Support | Quantization | LoRA |  Kunlun Graph |
|:------|:-------:|:------------:|:----:|:----------------------:|
| Qwen2 | âœ… | âœ…| âœ… | âœ… |
| Qwen2.5 | âœ… |âœ… | âœ… | âœ… |
| Qwen3 | âœ… |âœ… | âœ… | âœ… |
| Qwen3-Moe | âœ… | âœ… |  | âœ… |
| Qwen3-Next | âœ… | âœ… |  | âœ… |
| MiMo-V2-Flash | âœ… | âœ…| | âœ… |
| Llama2 | âœ… | âœ…|  âœ…| âœ… |
| Llama3 | âœ… |âœ… | âœ… | âœ… |
| Llama3.1 | âœ… |âœ… | | âœ… |
| gpt-oss | âœ… | âœ…| | |
| GLM4.5 | âœ… | âœ…| | âœ… |
| GLM4.5Air | âœ… |âœ… | | âœ… |
| GLM4.7 | âœ… | âœ…| | âœ… |
| GLM5 | âœ… | âœ…| | âœ… |
| Kimi-K2 | âœ… | âœ… | | âœ… |
| DeepSeek-R1 | âœ… | âœ… | | âœ… |
| DeepSeek-V3 | âœ… | âœ… | | âœ… |
| DeepSeek-V3.2 | âœ… | âœ… | | âœ… |

### Multimodal Language Models

| Model | Support | Quantization | LoRA |  Kunlun Graph |
|:------|:-------:|:------------:|:----:|:----------------------:|
| Qwen2-VL | âœ… | âœ…| | âœ… |
| Qwen2.5-VL | âœ… | âœ…| | âœ… |
| Qwen3-VL | âœ… | âœ…| | âœ… |
| Qwen3-VL-MoE | âœ… | âœ… | | âœ… |
| Qwen3-Omni-MoE | âœ… | | | âœ… |
| InternVL-2.5 | âœ… | | | âœ… |
| InternVL-3.5 | âœ… | | | âœ… |
| InternS1 | âœ… | | | âœ… |

---

## Performance Visualization ğŸš€

### High-performance computing at work: How different models perform on the Kunlun3 P800.

Current environment: 16-way concurrency, input/output size 2048.

![Models and tgs](./vllm_kunlun/patches/performance.png)

---

### Quick Start

#### Start an OpenAI-Compatible API Server

```bash
python -m vllm.entrypoints.openai.api_server \
    --host 0.0.0.0 \
    --port 8356 \
    --model <your-model-path> \
    --gpu-memory-utilization 0.9 \
    --trust-remote-code \
    --max-model-len 32768 \
    --tensor-parallel-size 1 \
    --dtype float16 \
    --max_num_seqs 128 \
    --max_num_batched_tokens 32768 \
    --block-size 128 \
    --distributed-executor-backend mp \
    --served-model-name <your-model-name>
```

#### Send a Request

```bash
curl http://localhost:8356/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "<your-model-name>",
    "messages": [{"role": "user", "content": "Hello!"}],
    "max_tokens": 512
  }'
```

### Version Matrix

| Version | Release Type | Documentation |
|---------|:------------:|:-------------:|
| v0.11.0 | Latest stable version | [Quick Start](https://vllm-kunlun.readthedocs.io/en/latest/quick_start.html) Â· [Installation](https://vllm-kunlun.readthedocs.io/en/latest/installation.html) |

---

## Architecture

```
vllm-kunlun/
â”œâ”€â”€ vllm_kunlun/              # Core plugin package
â”‚   â”œâ”€â”€ platforms/             # Kunlun XPU platform implementation
â”‚   â”œâ”€â”€ models/                # Model implementations (DeepSeek, Qwen, Llama, etc.)
â”‚   â”œâ”€â”€ ops/                   # Custom operators (attention, linear, sampling, etc.)
â”‚   â”‚   â”œâ”€â”€ attention/         # FlashMLA, paged attention, merge attention states
â”‚   â”‚   â”œâ”€â”€ fla/               # Flash linear attention operations
â”‚   â”‚   â””â”€â”€ sample/            # Sampling operators
â”‚   â”œâ”€â”€ v1/                    # vLLM V1 engine adaptations
â”‚   â”œâ”€â”€ compilation/           # Torch compile wrapper for Kunlun Graph
â”‚   â”œâ”€â”€ csrc/                  # C++ extensions (custom CUDA-compatible kernels)
â”‚   â””â”€â”€ config/                # Model configuration overrides
â”œâ”€â”€ tests/                     # Test suite
â”œâ”€â”€ docs/                      # Documentation (Sphinx-based, ReadTheDocs hosted)
â”œâ”€â”€ ci/                        # CI pipeline configurations
â”œâ”€â”€ setup.py                   # Legacy build script (with C++ extensions)
â””â”€â”€ pyproject.toml             # Modern Python build configuration (hatchling)
```

---

## Contributing

We welcome contributions from the community! Please read our [Contributing Guide](CONTRIBUTING.md) before submitting a PR.

### PR Classification

Use the following prefixes for PR titles:

- `[Attention]` â€” Attention mechanism features/optimizations
- `[Core]` â€” Core vllm-kunlun logic (platform, attention, communicators, model runner)
- `[Kernel]` â€” Compute kernels and ops
- `[Bugfix]` â€” Bug fixes
- `[Doc]` â€” Documentation improvements
- `[Test]` â€” Tests
- `[CI]` â€” CI/CD improvements
- `[Misc]` â€” Other changes

---

## Star History ğŸ”¥

We opened the project at Dec 8, 2025. We love open source and collaboration â¤ï¸

[![Star History Chart](https://api.star-history.com/svg?repos=baidu/vLLM-Kunlun&type=date&legend=bottom-right)](https://www.star-history.com/#baidu/vLLM-Kunlun&type=date&legend=bottom-right)

---

## Sponsors ğŸ‘‹

We sincerely appreciate the [**KunLunXin**](https://www.kunlunxin.com/) team for their support in providing XPU resources, which enabled efficient model adaptation debugging, comprehensive end-to-end testing, and broader model compatibility.

---

## License

Apache License 2.0, as found in the [LICENSE](./LICENSE) file.
