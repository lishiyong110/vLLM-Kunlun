#
# Copyright (c) 2025 Baidu, Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# This file is a part of the vllm-kunlun project.
#
"""
Kunlun-optimized VocabParallelEmbedding using vLLM's CustomOp.register_oot mechanism.

Design:
- Uses @CustomOp.register_oot to register Kunlun-optimized VocabParallelEmbedding
- This class automatically replaces the default implementation when instantiated
- Since KunlunPlatform uses _enum=PlatformEnum.CUDA, dispatch_forward() selects
  forward_cuda, so we implement forward_native (called by forward_cuda)

OOT Mechanism:
- When code calls VocabParallelEmbedding(...), vLLM's CustomOp.__new__ checks op_registry_oot
- If "VocabParallelEmbedding" is found in OOT registry, it returns KunlunVocabParallelEmbedding instance
- This is the official vLLM way to replace operators without modifying source code
"""

import logging
import sys

import torch
from vllm.distributed import tensor_model_parallel_all_reduce
from vllm.model_executor.custom_op import CustomOp
from vllm.model_executor.layers.vocab_parallel_embedding import VocabParallelEmbedding

logger = logging.getLogger("vllm_kunlun.ops.vocab_parallel_embedding")

# Track if OOT class has logged (for logging once)
_oot_vocab_embedding_init_logged = False


# =============================================================================
# Helper function for masked input computation
# =============================================================================


@torch.compile(dynamic=True, backend="aot_eager")
def get_masked_input_and_mask(
    input_: torch.Tensor,
    org_vocab_start_index: int,
    org_vocab_end_index: int,
    num_org_vocab_padding: int,
    added_vocab_start_index: int,
    added_vocab_end_index: int,
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Compute masked input and mask for vocabulary parallel embedding.

    torch.compile will fuse all of the pointwise ops below
    into a single kernel, making it very fast.
    """
    org_vocab_mask = (input_ >= org_vocab_start_index) & (input_ < org_vocab_end_index)
    added_vocab_mask = (input_ >= added_vocab_start_index) & (
        input_ < added_vocab_end_index
    )
    added_offset = (
        added_vocab_start_index
        - (org_vocab_end_index - org_vocab_start_index)
        - num_org_vocab_padding
    )
    valid_offset = (org_vocab_start_index * org_vocab_mask) + (
        added_offset * added_vocab_mask
    )
    vocab_mask = org_vocab_mask | added_vocab_mask
    input_ = vocab_mask * (input_ - valid_offset)
    return input_, ~vocab_mask


# =============================================================================
# OOT-registered Kunlun VocabParallelEmbedding class
# =============================================================================


@CustomOp.register_oot(name="VocabParallelEmbedding")
class KunlunVocabParallelEmbedding(VocabParallelEmbedding):
    """
    Kunlun-optimized VocabParallelEmbedding registered via OOT mechanism.

    This class replaces the default VocabParallelEmbedding when instantiated through
    vLLM's CustomOp registry. When code calls VocabParallelEmbedding(...), vLLM's
    CustomOp.__new__ checks op_registry_oot and returns KunlunVocabParallelEmbedding instance.
    """

    def __init__(self, *args, **kwargs):
        global _oot_vocab_embedding_init_logged
        super().__init__(*args, **kwargs)
        if not _oot_vocab_embedding_init_logged:
            logger.error(
                "[KunlunOOT] KunlunVocabParallelEmbedding.__init__ called (OOT instantiation)"
            )
            _oot_vocab_embedding_init_logged = True

    def forward_native(self, input_):
        """Kunlun-optimized forward_native implementation."""
        if self.tp_size > 1:
            # Build the mask using compiled function
            masked_input, input_mask = get_masked_input_and_mask(
                input_,
                self.shard_indices.org_vocab_start_index,
                self.shard_indices.org_vocab_end_index,
                self.shard_indices.num_org_vocab_padding,
                self.shard_indices.added_vocab_start_index,
                self.shard_indices.added_vocab_end_index,
            )
        else:
            masked_input = input_

        # Get the embeddings
        output_parallel = self.quant_method.embedding(self, masked_input.long())

        # Mask the output embedding
        if self.tp_size > 1:
            output_parallel.masked_fill_(input_mask.unsqueeze(-1), 0)

        # Reduce across all the model parallel GPUs
        output = tensor_model_parallel_all_reduce(output_parallel)
        return output


# Log that OOT registration is complete
print(
    "[KunlunOOT] Registered KunlunVocabParallelEmbedding via CustomOp.register_oot",
    file=sys.stderr,
    flush=True,
)
